{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"What is the MNE Study Template? \u00b6 The MNE Study Template is a full-flegded processing pipeline for your MEG and EEG data. It operates on data stored according to the Brain Imaging Data Structure (BIDS). The input is your raw data; the Study Template is configured using a simple, human-readable configuration file. When run, it will conduct preprocessing (filtering, artifact rejection), epoching, generation of evoked responses, contrasting of experimental conditions, time-frequency analysis, and source estimation. All intermediate results are saved to disk for later inspection, and an extensive report is generated. Analyses are conducted on individual (per-subject) as well as group level.","title":"Introduction"},{"location":"index.html#what-is-the-mne-study-template","text":"The MNE Study Template is a full-flegded processing pipeline for your MEG and EEG data. It operates on data stored according to the Brain Imaging Data Structure (BIDS). The input is your raw data; the Study Template is configured using a simple, human-readable configuration file. When run, it will conduct preprocessing (filtering, artifact rejection), epoching, generation of evoked responses, contrasting of experimental conditions, time-frequency analysis, and source estimation. All intermediate results are saved to disk for later inspection, and an extensive report is generated. Analyses are conducted on individual (per-subject) as well as group level.","title":"What is the MNE Study Template?"},{"location":"getting_started.html","text":"Prepare your dataset \u00b6 The Study Template only works with BIDS-formatted raw data. To find out more about BIDS and how to convert your data to the BIDS format, please see the documentation of MNE-BIDS . It is of great importance that the data is anonymized if you require anonymization, as the Study Template does not allow for anonymization. Why? This was a conscious design decision, not a technical limitation per se . If you think this decision should be reconsidered, please get in touch with the developers. faulty channels are marked as \"bad\". Why? While we do run automated bad channel detection in the Study Template, it is considered good practice to flag obviously problematic channels as such in the BIDS dataset. Adjust your configuration file \u00b6 The Study Template ships with a default configuration file, config.py . You need to create a copy of that configuration file and adjust all parameters that are relevant to your data processing and analysis. Info You should only need to touch the configuration file. None of the scripts should be edited. Run the Study Template \u00b6 Run the full Study Template by invoking python run.py all --config = /path/to/your/custom_config.py To only run the sensor-level, source-level, or report-generating steps, run: python run.py sensor --config = /path/to/your/custom_config.py # sensor-level python run.py source --config = /path/to/your/custom_config.py # source-level python run.py report --config = /path/to/your/custom_config.py # generate Reports","title":"Getting Started"},{"location":"getting_started.html#prepare-your-dataset","text":"The Study Template only works with BIDS-formatted raw data. To find out more about BIDS and how to convert your data to the BIDS format, please see the documentation of MNE-BIDS . It is of great importance that the data is anonymized if you require anonymization, as the Study Template does not allow for anonymization. Why? This was a conscious design decision, not a technical limitation per se . If you think this decision should be reconsidered, please get in touch with the developers. faulty channels are marked as \"bad\". Why? While we do run automated bad channel detection in the Study Template, it is considered good practice to flag obviously problematic channels as such in the BIDS dataset.","title":"Prepare your dataset"},{"location":"getting_started.html#adjust-your-configuration-file","text":"The Study Template ships with a default configuration file, config.py . You need to create a copy of that configuration file and adjust all parameters that are relevant to your data processing and analysis. Info You should only need to touch the configuration file. None of the scripts should be edited.","title":"Adjust your configuration file"},{"location":"getting_started.html#run-the-study-template","text":"Run the full Study Template by invoking python run.py all --config = /path/to/your/custom_config.py To only run the sensor-level, source-level, or report-generating steps, run: python run.py sensor --config = /path/to/your/custom_config.py # sensor-level python run.py source --config = /path/to/your/custom_config.py # source-level python run.py report --config = /path/to/your/custom_config.py # generate Reports","title":"Run the Study Template"},{"location":"install.html","text":"Install MNE-Python \u00b6 First, you need to make sure you have MNE-Python installed and working on your system. See the installation instructions . Install additional dependencies \u00b6 You will also need to install the a number of additional dependencies that are required to run the Study Template. Install for Python 3.8 and newer Run in your terminal: pip install mne-bids coloredlogs tqdm pandas scikit-learn json_tricks fire Install for older Python versions Run in your terminal: pip install mne-bids coloredlogs tqdm pandas json_tricks scikit-learn fire typing_extension Detailed list of dependencies mne-bids to operate on BIDS data coloredlogs for nicer logging output tqdm for progress bars pandas for table creation json_tricks for handling of some analysis output scikit-learn for decoding fire for the command line interface typing_extensions if you're using a Python version older than 3.8 Download the Study Template \u00b6 TODO","title":"Installation"},{"location":"install.html#install-mne-python","text":"First, you need to make sure you have MNE-Python installed and working on your system. See the installation instructions .","title":"Install MNE-Python"},{"location":"install.html#install-additional-dependencies","text":"You will also need to install the a number of additional dependencies that are required to run the Study Template. Install for Python 3.8 and newer Run in your terminal: pip install mne-bids coloredlogs tqdm pandas scikit-learn json_tricks fire Install for older Python versions Run in your terminal: pip install mne-bids coloredlogs tqdm pandas json_tricks scikit-learn fire typing_extension Detailed list of dependencies mne-bids to operate on BIDS data coloredlogs for nicer logging output tqdm for progress bars pandas for table creation json_tricks for handling of some analysis output scikit-learn for decoding fire for the command line interface typing_extensions if you're using a Python version older than 3.8","title":"Install additional dependencies"},{"location":"install.html#download-the-study-template","text":"TODO","title":"Download the Study Template"},{"location":"settings/basic.html","text":"study_name: str \u00b6 Specify the name of your study. It will be used to populate filenames for saving the analysis results. Example study_name = 'my-study' bids_root: Optional [ str ] \u00b6 Speficy the BIDS root directory. Pass an empty string or `None to use the value specified in the BIDS_ROOT environment variable instead. Raises an exception if the BIDS root has not been specified. Example bids_root = '/path/to/your/bids_root' # Use this to specify a path here. bids_root = None # Make use of the ``BIDS_ROOT`` environment variable. subjects_dir: Optional [ str ] \u00b6 Path to the directory that contains the MRI data files and their derivativesfor all subjects. Specifically, the subjects_dir is the $SUBJECTS_DIR used by the Freesurfer software. daysback: Optional [ int ] \u00b6 Warning This parameter will soon be removed! Anonymization should be done on the BIDS dataset before running the Study Template! If not None , apply a time shift to dates to adjust for limitateions of FIFF files. interactive: bool \u00b6 If True, the scripts will provide some interactive elements, such as figures. If running the scripts from a notebook or Spyder, run %matplotlib qt in the command line to open the figures in a separate window. crop: Optional [ tuple ] \u00b6 If tuple, (tmin, tmax) to crop the raw data If None (default), do not crop. sessions: Union [ Iterable , Literal [ 'all' ]] \u00b6 The sessions to process. task: str \u00b6 The task to process. runs: Union [ Iterable , Literal [ 'all' ]] \u00b6 The runs to process. acq: Optional [ str ] \u00b6 The BIDS acquisition entity. proc: Optional [ str ] \u00b6 The BIDS processing entity. rec: Optional [ str ] \u00b6 The BIDS recording entity. space: Optional [ str ] \u00b6 The BIDS space entity. subjects: Union [ Iterable [ str ], Literal [ 'all' ]] \u00b6 Subjects to analyze. If 'all' , include all subjects. To only include a subset of subjects, pass a list of their identifiers. Even if you plan on analyzing only a single subject, pass their identifier as a list. Please note that if you intend to EXCLUDE only a few subjects, you should consider setting subjects = 'all' and adding the identifiers of the excluded subjects to exclude_subjects (see next section). Example subjects = 'all' # Include all subjects. subjects = [ '05' ] # Only include subject 05. subjects = [ '01' , '02' ] # Only include subjects 01 and 02. exclude_subjects: Iterable [ str ] \u00b6 Specify subjects to exclude from analysis. The MEG empty-room mock-subject is automatically excluded from regular analysis. Good Practice / Advice Keep track of the criteria leading you to exclude a participant (e.g. too many movements, missing blocks, aborted experiment, did not understand the instructions, etc, ...) The emptyroom subject will be excluded automatically. process_er: bool \u00b6 Whether to apply the same pre-processing steps to the empty-room data as to the experimental data (up until including frequency filtering). This is required if you wish to use the empty-room recording to estimate noise covariance (via noise_cov='emptyroom' ). The empty-room recording corresponding to the processed experimental data will be retrieved automatically. ch_types: Iterable [ Literal [ 'meg' , 'mag' , 'grad' , 'eeg' ]] \u00b6 The channel types to consider. Info Currently, MEG and EEG data cannot be processed together. Example # Use EEG channels: ch_types = [ 'eeg' ] # Use magnetometer and gradiometer MEG channels: ch_types = [ 'mag' , 'grad' ] # Currently does not work and will raise an error message: ch_types = [ 'meg' , 'eeg' ] data_type: Optional [ Literal [ 'meg' , 'eeg' ]] \u00b6 The BIDS data type. For MEG recordings, this will usually be 'meg'; and for EEG, 'eeg'. However, if your dataset contains simultaneous recordings of MEG and EEG, stored in a single file, you will typically need to set this to 'meg'. If None , we will assume that the data type matches the channel type. Example The dataset contains simultaneous recordings of MEG and EEG, and we only wish to process the EEG data, which is stored inside the MEG files: ch_types = [ 'eeg' ] data_type = 'eeg' The dataset contains simultaneous recordings of MEG and EEG, and we only wish to process the gradiometer data: ch_types = [ 'grad' ] data_type = 'meg' # or data_type = None The dataset contains only EEG data: ch_types = [ 'eeg' ] data_type = 'eeg' # or data_type = None eeg_template_montage: Optional [ str ] \u00b6 In situations where you wish to process EEG data and no individual digitization points (measured channel locations) are available, you can apply a \"template\" montage. This means we will assume the EEG cap was placed either according to an international system like 10/20, or as suggested by the cap manufacturers in their respective manual. Please be aware that the actual cap placement most likely deviated somewhat from the template, and, therefore, source reconstruction may be impaired. If None , do not apply a template montage. If a string, must be the name of a built-in template montage in MNE-Python. You can find an overview of supported template montages at https://mne.tools/stable/generated/mne.channels.make_standard_montage.html Example Do not apply template montage: eeg_template_montage = None Apply 64-channel Biosemi 10/20 template montage: eeg_template_montage = 'biosemi64'","title":"Basic settings"},{"location":"settings/basic.html#config.study_name","text":"Specify the name of your study. It will be used to populate filenames for saving the analysis results. Example study_name = 'my-study'","title":"study_name"},{"location":"settings/basic.html#config.bids_root","text":"Speficy the BIDS root directory. Pass an empty string or `None to use the value specified in the BIDS_ROOT environment variable instead. Raises an exception if the BIDS root has not been specified. Example bids_root = '/path/to/your/bids_root' # Use this to specify a path here. bids_root = None # Make use of the ``BIDS_ROOT`` environment variable.","title":"bids_root"},{"location":"settings/basic.html#config.subjects_dir","text":"Path to the directory that contains the MRI data files and their derivativesfor all subjects. Specifically, the subjects_dir is the $SUBJECTS_DIR used by the Freesurfer software.","title":"subjects_dir"},{"location":"settings/basic.html#config.daysback","text":"Warning This parameter will soon be removed! Anonymization should be done on the BIDS dataset before running the Study Template! If not None , apply a time shift to dates to adjust for limitateions of FIFF files.","title":"daysback"},{"location":"settings/basic.html#config.interactive","text":"If True, the scripts will provide some interactive elements, such as figures. If running the scripts from a notebook or Spyder, run %matplotlib qt in the command line to open the figures in a separate window.","title":"interactive"},{"location":"settings/basic.html#config.crop","text":"If tuple, (tmin, tmax) to crop the raw data If None (default), do not crop.","title":"crop"},{"location":"settings/basic.html#config.sessions","text":"The sessions to process.","title":"sessions"},{"location":"settings/basic.html#config.task","text":"The task to process.","title":"task"},{"location":"settings/basic.html#config.runs","text":"The runs to process.","title":"runs"},{"location":"settings/basic.html#config.acq","text":"The BIDS acquisition entity.","title":"acq"},{"location":"settings/basic.html#config.proc","text":"The BIDS processing entity.","title":"proc"},{"location":"settings/basic.html#config.rec","text":"The BIDS recording entity.","title":"rec"},{"location":"settings/basic.html#config.space","text":"The BIDS space entity.","title":"space"},{"location":"settings/basic.html#config.subjects","text":"Subjects to analyze. If 'all' , include all subjects. To only include a subset of subjects, pass a list of their identifiers. Even if you plan on analyzing only a single subject, pass their identifier as a list. Please note that if you intend to EXCLUDE only a few subjects, you should consider setting subjects = 'all' and adding the identifiers of the excluded subjects to exclude_subjects (see next section). Example subjects = 'all' # Include all subjects. subjects = [ '05' ] # Only include subject 05. subjects = [ '01' , '02' ] # Only include subjects 01 and 02.","title":"subjects"},{"location":"settings/basic.html#config.exclude_subjects","text":"Specify subjects to exclude from analysis. The MEG empty-room mock-subject is automatically excluded from regular analysis. Good Practice / Advice Keep track of the criteria leading you to exclude a participant (e.g. too many movements, missing blocks, aborted experiment, did not understand the instructions, etc, ...) The emptyroom subject will be excluded automatically.","title":"exclude_subjects"},{"location":"settings/basic.html#config.process_er","text":"Whether to apply the same pre-processing steps to the empty-room data as to the experimental data (up until including frequency filtering). This is required if you wish to use the empty-room recording to estimate noise covariance (via noise_cov='emptyroom' ). The empty-room recording corresponding to the processed experimental data will be retrieved automatically.","title":"process_er"},{"location":"settings/basic.html#config.ch_types","text":"The channel types to consider. Info Currently, MEG and EEG data cannot be processed together. Example # Use EEG channels: ch_types = [ 'eeg' ] # Use magnetometer and gradiometer MEG channels: ch_types = [ 'mag' , 'grad' ] # Currently does not work and will raise an error message: ch_types = [ 'meg' , 'eeg' ]","title":"ch_types"},{"location":"settings/basic.html#config.data_type","text":"The BIDS data type. For MEG recordings, this will usually be 'meg'; and for EEG, 'eeg'. However, if your dataset contains simultaneous recordings of MEG and EEG, stored in a single file, you will typically need to set this to 'meg'. If None , we will assume that the data type matches the channel type. Example The dataset contains simultaneous recordings of MEG and EEG, and we only wish to process the EEG data, which is stored inside the MEG files: ch_types = [ 'eeg' ] data_type = 'eeg' The dataset contains simultaneous recordings of MEG and EEG, and we only wish to process the gradiometer data: ch_types = [ 'grad' ] data_type = 'meg' # or data_type = None The dataset contains only EEG data: ch_types = [ 'eeg' ] data_type = 'eeg' # or data_type = None","title":"data_type"},{"location":"settings/basic.html#config.eeg_template_montage","text":"In situations where you wish to process EEG data and no individual digitization points (measured channel locations) are available, you can apply a \"template\" montage. This means we will assume the EEG cap was placed either according to an international system like 10/20, or as suggested by the cap manufacturers in their respective manual. Please be aware that the actual cap placement most likely deviated somewhat from the template, and, therefore, source reconstruction may be impaired. If None , do not apply a template montage. If a string, must be the name of a built-in template montage in MNE-Python. You can find an overview of supported template montages at https://mne.tools/stable/generated/mne.channels.make_standard_montage.html Example Do not apply template montage: eeg_template_montage = None Apply 64-channel Biosemi 10/20 template montage: eeg_template_montage = 'biosemi64'","title":"eeg_template_montage"},{"location":"settings/continuous_data/autobads.html","text":"Warning This functionality will soon be removed from the Study Template, and will be integrated into MNE-BIDS. \"Bad\", i.e. flat and overly noisy channels, can be automatically detected using a procedure inspired by the commercial MaxFilter by Elekta. First, a copy of the data is low-pass filtered at 40 Hz. Then, channels with unusually low variability are flagged as \"flat\", while channels with excessively high variability are flagged as \"noisy\". Flat and noisy channels are marked as \"bad\" and excluded from subsequent analysis. See :func: mne.preprocssessing.find_bad_channels_maxwell for more information on this procedure. The list of bad channels detected through this procedure will be merged with the list of bad channels already present in the dataset, if any. find_flat_channels_meg: bool \u00b6 Auto-detect \"flat\" channels (i.e. those with unusually low variability) and mark them as bad. find_noisy_channels_meg: bool \u00b6 Auto-detect \"noisy\" channels and mark them as bad.","title":"Bad channel detection"},{"location":"settings/continuous_data/autobads.html#config.find_flat_channels_meg","text":"Auto-detect \"flat\" channels (i.e. those with unusually low variability) and mark them as bad.","title":"find_flat_channels_meg"},{"location":"settings/continuous_data/autobads.html#config.find_noisy_channels_meg","text":"Auto-detect \"noisy\" channels and mark them as bad.","title":"find_noisy_channels_meg"},{"location":"settings/continuous_data/filter.html","text":"It is typically better to set your filtering properties on the raw data so as to avoid what we call border (or edge) effects. If you use this pipeline for evoked responses, you could consider a low-pass filter cut-off of h_freq = 40 Hz and possibly a high-pass filter cut-off of l_freq = 1 Hz so you would preserve only the power in the 1Hz to 40 Hz band. Note that highpass filtering is not necessarily recommended as it can distort waveforms of evoked components, or simply wash out any low frequency that can may contain brain signal. It can also act as a replacement for baseline correction in Epochs. See below. If you use this pipeline for time-frequency analysis, a default filtering coult be a high-pass filter cut-off of l_freq = 1 Hz a low-pass filter cut-off of h_freq = 120 Hz so you would preserve only the power in the 1Hz to 120 Hz band. If you need more fancy analysis, you are already likely past this kind of tips! \ud83d\ude07 l_freq: float \u00b6 The low-frequency cut-off in the highpass filtering step. Keep it None if no highpass filtering should be applied. h_freq: float \u00b6 The high-frequency cut-off in the lowpass filtering step. Keep it None if no lowpass filtering should be applied.","title":"Filtering"},{"location":"settings/continuous_data/filter.html#config.l_freq","text":"The low-frequency cut-off in the highpass filtering step. Keep it None if no highpass filtering should be applied.","title":"l_freq"},{"location":"settings/continuous_data/filter.html#config.h_freq","text":"The high-frequency cut-off in the lowpass filtering step. Keep it None if no lowpass filtering should be applied.","title":"h_freq"},{"location":"settings/continuous_data/maxfilter.html","text":"use_maxwell_filter: bool \u00b6 Whether or not to use Maxwell filtering to preprocess the data. Warning If the data were recorded with internal active compensation (MaxShield), they need to be run through Maxwell filter to avoid distortions. Bad channels need to be set through BIDS channels.tsv and / or via the find_flat_channels_meg and find_noisy_channels_meg options above before applying Maxwell filter. mf_st_duration: Optional [ float ] \u00b6 There are two kinds of maxfiltering: SSS (signal space separation) and tSSS (temporal signal space separation) (see Taulu et al., 2004 ). If not None, apply spatiotemporal SSS (tSSS) with specified buffer duration (in seconds). MaxFilter\u2122's default is 10.0 seconds in v2.2. Spatiotemporal SSS acts as implicitly as a high-pass filter where the cut-off frequency is 1/st_dur Hz. For this (and other) reasons, longer buffers are generally better as long as your system can handle the higher memory usage. To ensure that each window is processed identically, choose a buffer length that divides evenly into your data. Any data at the trailing edge that doesn't fit evenly into a whole buffer window will be lumped into the previous buffer. Good Practice / Advice If you are interested in low frequency activity (<0.1Hz), avoid using tSSS and set mf_st_duration to None . If you are interested in low frequency above 0.1 Hz, you can use the default mf_st_duration to 10 s, meaning it acts like a 0.1 Hz high-pass filter. Example mf_st_duration = None mf_st_duration = 10. # to apply tSSS with 0.1Hz highpass filter. mf_head_origin \u00b6 mf_head_origin : array-like, shape (3,) | 'auto' Origin of internal and external multipolar moment space in meters. If 'auto', it will be estimated from headshape points. If automatic fitting fails (e.g., due to having too few digitization points), consider separately calling the fitting function with different options or specifying the origin manually. Example mf_head_origin = 'auto' mf_reference_run: Optional [ str ] \u00b6 Despite all possible care to avoid movements in the MEG, the participant will likely slowly drift down from the Dewar or slightly shift the head around in the course of the recording session. Hence, to take this into account, we are realigning all data to a single position. For this, you need to define a reference run (typically the one in the middle of the recording session). Which run to take as the reference for adjusting the head position of all runs. If None , pick the first run. Example mf_reference_run = '01' # Use run \"01\"","title":"Maxwell filter"},{"location":"settings/continuous_data/maxfilter.html#config.use_maxwell_filter","text":"Whether or not to use Maxwell filtering to preprocess the data. Warning If the data were recorded with internal active compensation (MaxShield), they need to be run through Maxwell filter to avoid distortions. Bad channels need to be set through BIDS channels.tsv and / or via the find_flat_channels_meg and find_noisy_channels_meg options above before applying Maxwell filter.","title":"use_maxwell_filter"},{"location":"settings/continuous_data/maxfilter.html#config.mf_st_duration","text":"There are two kinds of maxfiltering: SSS (signal space separation) and tSSS (temporal signal space separation) (see Taulu et al., 2004 ). If not None, apply spatiotemporal SSS (tSSS) with specified buffer duration (in seconds). MaxFilter\u2122's default is 10.0 seconds in v2.2. Spatiotemporal SSS acts as implicitly as a high-pass filter where the cut-off frequency is 1/st_dur Hz. For this (and other) reasons, longer buffers are generally better as long as your system can handle the higher memory usage. To ensure that each window is processed identically, choose a buffer length that divides evenly into your data. Any data at the trailing edge that doesn't fit evenly into a whole buffer window will be lumped into the previous buffer. Good Practice / Advice If you are interested in low frequency activity (<0.1Hz), avoid using tSSS and set mf_st_duration to None . If you are interested in low frequency above 0.1 Hz, you can use the default mf_st_duration to 10 s, meaning it acts like a 0.1 Hz high-pass filter. Example mf_st_duration = None mf_st_duration = 10. # to apply tSSS with 0.1Hz highpass filter.","title":"mf_st_duration"},{"location":"settings/continuous_data/maxfilter.html#config.mf_head_origin","text":"mf_head_origin : array-like, shape (3,) | 'auto' Origin of internal and external multipolar moment space in meters. If 'auto', it will be estimated from headshape points. If automatic fitting fails (e.g., due to having too few digitization points), consider separately calling the fitting function with different options or specifying the origin manually. Example mf_head_origin = 'auto'","title":"mf_head_origin"},{"location":"settings/continuous_data/maxfilter.html#config.mf_reference_run","text":"Despite all possible care to avoid movements in the MEG, the participant will likely slowly drift down from the Dewar or slightly shift the head around in the course of the recording session. Hence, to take this into account, we are realigning all data to a single position. For this, you need to define a reference run (typically the one in the middle of the recording session). Which run to take as the reference for adjusting the head position of all runs. If None , pick the first run. Example mf_reference_run = '01' # Use run \"01\"","title":"mf_reference_run"},{"location":"settings/continuous_data/resample.html","text":"If you have acquired data with a very high sampling frequency (e.g. 2 kHz) you will likely want to downsample to lighten up the size of the files you are working with (pragmatics) If you are interested in typical analysis (up to 120 Hz) you can typically resample your data down to 500 Hz without preventing reliable time-frequency exploration of your data. resample_sfreq: Optional [ float ] \u00b6 Specifies at which sampling frequency the data should be resampled. If None then no resampling will be done. Example resample_sfreq = None # no resampling resample_sfreq = 500 # resample to 500Hz decim: int \u00b6 Says how much to decimate data at the epochs level. It is typically an alternative to the resample_sfreq parameter that can be used for resampling raw data. 1 means no decimation. Good Practice / Advice Decimation requires to lowpass filtered the data to avoid aliasing. Note that using decimation is much faster than resampling. Example decim = 1 # no decimation decim = 4 # decimate by 4 ie devide sampling frequency by 4","title":"Resampling"},{"location":"settings/continuous_data/resample.html#config.resample_sfreq","text":"Specifies at which sampling frequency the data should be resampled. If None then no resampling will be done. Example resample_sfreq = None # no resampling resample_sfreq = 500 # resample to 500Hz","title":"resample_sfreq"},{"location":"settings/continuous_data/resample.html#config.decim","text":"Says how much to decimate data at the epochs level. It is typically an alternative to the resample_sfreq parameter that can be used for resampling raw data. 1 means no decimation. Good Practice / Advice Decimation requires to lowpass filtered the data to avoid aliasing. Note that using decimation is much faster than resampling. Example decim = 1 # no decimation decim = 4 # decimate by 4 ie devide sampling frequency by 4","title":"decim"},{"location":"settings/continuous_data/stim_artifact.html","text":"When using electric stimulation systems, e.g. for median nerve or index stimulation, it is frequent to have a stimulation artifact. This option allows to fix it by linear interpolation early in the pipeline on the raw data. fix_stim_artifact: bool \u00b6 Apply interpolation to fix stimulation artifact. Example fix_stim_artifact = False stim_artifact_tmin: float \u00b6 Start time of the interpolation window in seconds. Example stim_artifact_tmin = 0. # on stim onset stim_artifact_tmax: float \u00b6 End time of the interpolation window in seconds. Example stim_artifact_tmax = 0.01 # up to 10ms post-stimulation","title":"Stimulation artifact"},{"location":"settings/continuous_data/stim_artifact.html#config.fix_stim_artifact","text":"Apply interpolation to fix stimulation artifact. Example fix_stim_artifact = False","title":"fix_stim_artifact"},{"location":"settings/continuous_data/stim_artifact.html#config.stim_artifact_tmin","text":"Start time of the interpolation window in seconds. Example stim_artifact_tmin = 0. # on stim onset","title":"stim_artifact_tmin"},{"location":"settings/continuous_data/stim_artifact.html#config.stim_artifact_tmax","text":"End time of the interpolation window in seconds. Example stim_artifact_tmax = 0.01 # up to 10ms post-stimulation","title":"stim_artifact_tmax"},{"location":"settings/epoched_data/artifacts.html","text":"Good Practice / Advice Have a look at your raw data and train yourself to detect a blink, a heart beat and an eye movement. You can do a quick average of blink data and check what the amplitude looks like. reject: Optional [ dict ] \u00b6 The rejection limits to mark epochs as bads. This allows to remove strong transient artifacts. If you want to reject and retrieve blinks or ECG artifacts later, e.g. with ICA, don't specify a value for the EOG and ECG channels, respectively (see examples below). Pass None to avoid automated epoch rejection based on amplitude. Note These numbers tend to vary between subjects.. You might want to consider using the autoreject method by Jas et al. 2018. See https://autoreject.github.io Example reject = { 'grad' : 4000e-13 , 'mag' : 4e-12 , 'eog' : 150e-6 } reject = { 'grad' : 4000e-13 , 'mag' : 4e-12 , 'eeg' : 200e-6 } reject = None","title":"Amplitude-based artifact removal"},{"location":"settings/epoched_data/artifacts.html#config.reject","text":"The rejection limits to mark epochs as bads. This allows to remove strong transient artifacts. If you want to reject and retrieve blinks or ECG artifacts later, e.g. with ICA, don't specify a value for the EOG and ECG channels, respectively (see examples below). Pass None to avoid automated epoch rejection based on amplitude. Note These numbers tend to vary between subjects.. You might want to consider using the autoreject method by Jas et al. 2018. See https://autoreject.github.io Example reject = { 'grad' : 4000e-13 , 'mag' : 4e-12 , 'eog' : 150e-6 } reject = { 'grad' : 4000e-13 , 'mag' : 4e-12 , 'eeg' : 200e-6 } reject = None","title":"reject"},{"location":"settings/epoched_data/epochs.html","text":"rename_events: dict \u00b6 A dictionary specifying which events in the BIDS dataset to rename upon loading, and before processing begins. Pass an empty dictionary to not perform any renaming. Example Rename audio_left in the BIDS dataset to audio/left in the pipeline: rename_events = { 'audio_left' : 'audio/left' } conditions: Iterable [ str ] \u00b6 The condition names to consider. This can either be name of the experimental condition as specified in the BIDS events.tsv file; or the name of condition grouped , if the condition names contain the (MNE-specific) group separator, / . See the Subselecting epochs tutorial for more information. Example conditions = [ 'auditory/left' , 'visual/left' ] conditions = [ 'auditory/left' , 'auditory/right' ] conditions = [ 'auditory' ] # All \"auditory\" conditions (left AND right) conditions = [ 'auditory' , 'visual' ] conditions = [ 'left' , 'right' ] tmin \u00b6 The beginning of an epoch, relative to the respective event, in seconds. Example tmin = - 0.2 # take 200ms before event onset. tmax: float \u00b6 Example tmax = 0.5 # take 500ms after event onset. baseline: Optional [ tuple ] \u00b6 Specifies how to baseline-correct the epochs; if None , no baseline correction is applied. Example baseline = ( None , 0 ) # baseline between tmin and 0","title":"Epoching"},{"location":"settings/epoched_data/epochs.html#config.rename_events","text":"A dictionary specifying which events in the BIDS dataset to rename upon loading, and before processing begins. Pass an empty dictionary to not perform any renaming. Example Rename audio_left in the BIDS dataset to audio/left in the pipeline: rename_events = { 'audio_left' : 'audio/left' }","title":"rename_events"},{"location":"settings/epoched_data/epochs.html#config.conditions","text":"The condition names to consider. This can either be name of the experimental condition as specified in the BIDS events.tsv file; or the name of condition grouped , if the condition names contain the (MNE-specific) group separator, / . See the Subselecting epochs tutorial for more information. Example conditions = [ 'auditory/left' , 'visual/left' ] conditions = [ 'auditory/left' , 'auditory/right' ] conditions = [ 'auditory' ] # All \"auditory\" conditions (left AND right) conditions = [ 'auditory' , 'visual' ] conditions = [ 'left' , 'right' ]","title":"conditions"},{"location":"settings/epoched_data/epochs.html#config.tmin","text":"The beginning of an epoch, relative to the respective event, in seconds. Example tmin = - 0.2 # take 200ms before event onset.","title":"tmin"},{"location":"settings/epoched_data/epochs.html#config.tmax","text":"Example tmax = 0.5 # take 500ms after event onset.","title":"tmax"},{"location":"settings/epoched_data/epochs.html#config.baseline","text":"Specifies how to baseline-correct the epochs; if None , no baseline correction is applied. Example baseline = ( None , 0 ) # baseline between tmin and 0","title":"baseline"},{"location":"settings/epoched_data/inverse.html","text":"spacing: str \u00b6 The spacing to use. Can be 'ico#' for a recursively subdivided icosahedron, 'oct#' for a recursively subdivided octahedron, 'all' for all points, or an integer to use appoximate distance-based spacing (in mm). mindist: float \u00b6 Exclude points closer than this distance (mm) to the bounding surface. loose: Union [ float , Literal [ 'auto' ]] \u00b6 Value that weights the source variances of the dipole components that are parallel (tangential) to the cortical surface. If loose is 0 then the solution is computed with fixed orientation, and fixed must be True or \"auto\". If loose is 1, it corresponds to free orientations. The default value ('auto') is set to 0.2 for surface-oriented source space and set to 1.0 for volumetric, discrete, or mixed source spaces, unless fixed is True in which case the value 0. is used. depth: Union [ float , dict ] \u00b6 If float (default 0.8), it acts as the depth weighting exponent ( exp ) to use (must be between 0 and 1). None is equivalent to 0, meaning no depth weighting is performed. Can also be a dict containing additional keyword arguments to pass to :func: mne.forward.compute_depth_prior (see docstring for details and defaults). inverse_method: Literal [ 'MNE' , 'dSPM' , 'sLORETA' , 'eLORETA' ] \u00b6 Use minimum norm, dSPM (default), sLORETA, or eLORETA to calculate the inverse solution. noise_cov: Union [ tuple , Literal [ 'emptyroom' ]] \u00b6 Specify how to estimate the noise covariance matrix, which is used in inverse modeling. If a tuple, it takes the form (tmin, tmax) with the time specified in seconds. If the first value of the tuple is None , the considered period starts at the beginning of the epoch. If the second value of the tuple is None , the considered period ends at the end of the epoch. The default, (None, 0) , includes the entire period before the event, which is typically the pre-stimulus period. If emptyroom , the noise covariance matrix will be estimated from an empty-room MEG recording. The empty-room recording will be automatically selected based on recording date and time. Please note that when processing data that contains EEG channels, the noise covariance can ONLY be estimated from the pre-stimulus period. Example Use the period from start of the epoch until 100 ms before the experimental event: noise_cov = ( None , - 0.1 ) Use the time period from the experimental event until the end of the epoch: noise_cov = ( 0 , None ) Use an empty-room recording: noise_cov = 'emptyroom' smooth: Optional [ int ] \u00b6 Number of iterations for the smoothing of the surface data. If None, smooth is automatically defined to fill the surface with non-zero values. The default is spacing=None.","title":"Source estimation"},{"location":"settings/epoched_data/inverse.html#config.spacing","text":"The spacing to use. Can be 'ico#' for a recursively subdivided icosahedron, 'oct#' for a recursively subdivided octahedron, 'all' for all points, or an integer to use appoximate distance-based spacing (in mm).","title":"spacing"},{"location":"settings/epoched_data/inverse.html#config.mindist","text":"Exclude points closer than this distance (mm) to the bounding surface.","title":"mindist"},{"location":"settings/epoched_data/inverse.html#config.loose","text":"Value that weights the source variances of the dipole components that are parallel (tangential) to the cortical surface. If loose is 0 then the solution is computed with fixed orientation, and fixed must be True or \"auto\". If loose is 1, it corresponds to free orientations. The default value ('auto') is set to 0.2 for surface-oriented source space and set to 1.0 for volumetric, discrete, or mixed source spaces, unless fixed is True in which case the value 0. is used.","title":"loose"},{"location":"settings/epoched_data/inverse.html#config.depth","text":"If float (default 0.8), it acts as the depth weighting exponent ( exp ) to use (must be between 0 and 1). None is equivalent to 0, meaning no depth weighting is performed. Can also be a dict containing additional keyword arguments to pass to :func: mne.forward.compute_depth_prior (see docstring for details and defaults).","title":"depth"},{"location":"settings/epoched_data/inverse.html#config.inverse_method","text":"Use minimum norm, dSPM (default), sLORETA, or eLORETA to calculate the inverse solution.","title":"inverse_method"},{"location":"settings/epoched_data/inverse.html#config.noise_cov","text":"Specify how to estimate the noise covariance matrix, which is used in inverse modeling. If a tuple, it takes the form (tmin, tmax) with the time specified in seconds. If the first value of the tuple is None , the considered period starts at the beginning of the epoch. If the second value of the tuple is None , the considered period ends at the end of the epoch. The default, (None, 0) , includes the entire period before the event, which is typically the pre-stimulus period. If emptyroom , the noise covariance matrix will be estimated from an empty-room MEG recording. The empty-room recording will be automatically selected based on recording date and time. Please note that when processing data that contains EEG channels, the noise covariance can ONLY be estimated from the pre-stimulus period. Example Use the period from start of the epoch until 100 ms before the experimental event: noise_cov = ( None , - 0.1 ) Use the time period from the experimental event until the end of the epoch: noise_cov = ( 0 , None ) Use an empty-room recording: noise_cov = 'emptyroom'","title":"noise_cov"},{"location":"settings/epoched_data/inverse.html#config.smooth","text":"Number of iterations for the smoothing of the surface data. If None, smooth is automatically defined to fill the surface with non-zero values. The default is spacing=None.","title":"smooth"},{"location":"settings/epoched_data/ssp_ica.html","text":"use_ssp: bool \u00b6 Whether signal-space projection should be used or not. use_ica: bool \u00b6 Whether independent component analysis should be used or not. ica_algorithm: Literal [ 'picard' , 'fastica' , 'extended_infomax' ] \u00b6 The ICA algorithm to use. ica_l_freq: Optional [ float ] \u00b6 The cutoff frequency of the high-pass filter to apply before running ICA. Using a relatively high cutoff like 1 Hz will remove slow drifts from the data, yielding improved ICA results. Set to None to not apply an additional high-pass filter. Note The filter will be applied to raw data which was already filtered according to the l_freq and h_freq settings. After filtering, the data will be epoched, and the epochs will be submitted to ICA. ica_max_iterations: int \u00b6 Maximum number of iterations to decompose the data into independent components. A low number means to finish earlier, but the consequence is that the algorithm may not have finished converging. To ensure convergence, pick a high number here (e.g. 3000); yet the algorithm will terminate as soon as it determines that is has successfully converged, and not necessarily exhaust the maximum number of iterations. Note that the default of 200 seems to be sufficient for Picard in many datasets, because it converges quicker than the other algorithms; but e.g. for FastICA, this limit may be too low to achieve convergence. ica_n_components: Union [ float , int ] \u00b6 MNE conducts ICA as a sort of a two-step procedure: First, a PCA is run on the data (trying to exclude zero-valued components in rank-deficient data); and in the second step, the principal componenets are passed to the actual ICA. You can select how many of the total principal components to pass to ICA \u2013 it can be all or just a subset. This determines how many independent components to fit, and can be controlled via this setting. If int, specifies the number of principal components that are passed to the ICA algorithm, which will be the number of independent components to fit. It must not be greater than the rank of your data (which is typically the number of channels, but may be less in some cases). If float between 0 and 1, all principal components with cumulative explained variance less than the value specified here will be passed to ICA. If None , all principal components will be used. This setting may drastically alter the time required to compute ICA. ica_decim: Optional [ int ] \u00b6 The decimation parameter to compute ICA. If 5 it means that 1 every 5 sample is used by ICA solver. The higher the faster it is to run but the less data you have to compute a good ICA. Set to 1 or None to not perform any decimation. ica_ctps_ecg_threshold: float \u00b6 The threshold parameter passed to find_bads_ecg method. ica_eog_threshold: float \u00b6 The threshold to use during automated EOG classification. Lower values mean that more ICs will be identified as EOG-related. If too low, the false-alarm rate increases dramatically.","title":"SSP & ICA"},{"location":"settings/epoched_data/ssp_ica.html#config.use_ssp","text":"Whether signal-space projection should be used or not.","title":"use_ssp"},{"location":"settings/epoched_data/ssp_ica.html#config.use_ica","text":"Whether independent component analysis should be used or not.","title":"use_ica"},{"location":"settings/epoched_data/ssp_ica.html#config.ica_algorithm","text":"The ICA algorithm to use.","title":"ica_algorithm"},{"location":"settings/epoched_data/ssp_ica.html#config.ica_l_freq","text":"The cutoff frequency of the high-pass filter to apply before running ICA. Using a relatively high cutoff like 1 Hz will remove slow drifts from the data, yielding improved ICA results. Set to None to not apply an additional high-pass filter. Note The filter will be applied to raw data which was already filtered according to the l_freq and h_freq settings. After filtering, the data will be epoched, and the epochs will be submitted to ICA.","title":"ica_l_freq"},{"location":"settings/epoched_data/ssp_ica.html#config.ica_max_iterations","text":"Maximum number of iterations to decompose the data into independent components. A low number means to finish earlier, but the consequence is that the algorithm may not have finished converging. To ensure convergence, pick a high number here (e.g. 3000); yet the algorithm will terminate as soon as it determines that is has successfully converged, and not necessarily exhaust the maximum number of iterations. Note that the default of 200 seems to be sufficient for Picard in many datasets, because it converges quicker than the other algorithms; but e.g. for FastICA, this limit may be too low to achieve convergence.","title":"ica_max_iterations"},{"location":"settings/epoched_data/ssp_ica.html#config.ica_n_components","text":"MNE conducts ICA as a sort of a two-step procedure: First, a PCA is run on the data (trying to exclude zero-valued components in rank-deficient data); and in the second step, the principal componenets are passed to the actual ICA. You can select how many of the total principal components to pass to ICA \u2013 it can be all or just a subset. This determines how many independent components to fit, and can be controlled via this setting. If int, specifies the number of principal components that are passed to the ICA algorithm, which will be the number of independent components to fit. It must not be greater than the rank of your data (which is typically the number of channels, but may be less in some cases). If float between 0 and 1, all principal components with cumulative explained variance less than the value specified here will be passed to ICA. If None , all principal components will be used. This setting may drastically alter the time required to compute ICA.","title":"ica_n_components"},{"location":"settings/epoched_data/ssp_ica.html#config.ica_decim","text":"The decimation parameter to compute ICA. If 5 it means that 1 every 5 sample is used by ICA solver. The higher the faster it is to run but the less data you have to compute a good ICA. Set to 1 or None to not perform any decimation.","title":"ica_decim"},{"location":"settings/epoched_data/ssp_ica.html#config.ica_ctps_ecg_threshold","text":"The threshold parameter passed to find_bads_ecg method.","title":"ica_ctps_ecg_threshold"},{"location":"settings/epoched_data/ssp_ica.html#config.ica_eog_threshold","text":"The threshold to use during automated EOG classification. Lower values mean that more ICs will be identified as EOG-related. If too low, the false-alarm rate increases dramatically.","title":"ica_eog_threshold"},{"location":"settings/epoched_data/statistics.html","text":"contrasts: Iterable [ tuple ] \u00b6 The conditions to contrast via a subtraction of ERPs / ERFs. Each tuple in the list corresponds to one contrast. The condition names must be specified in conditions above. Pass an empty list to avoid calculation of contrasts. Example Contrast the \"left\" and the \"right\" conditions by calculating left - right at every time point of the evoked responses: conditions = [ 'left' , 'right' ] contrasts = [( 'left' , 'right' )] # Note we pass a tuple inside the list! Contrast the \"left\" and the \"right\" conditions within the \"auditory\" and the \"visual\" modality, and \"auditory\" vs \"visual\" regardless of side: conditions = [ 'auditory/left' , 'auditory/right' , 'visual/left' , 'visual/right' ] contrasts = [( 'auditory/left' , 'auditory/right' ), ( 'visual/left' , 'visual/right' ), ( 'auditory' , 'visual' )] decode: bool \u00b6 Whether to perform decoding (MVPA) on the contrasts specified above as \"contrasts\". MVPA will be performed on the level of individual epochs. decoding_metric: str \u00b6 The metric to use for cross-validation. It can be 'roc_auc' or 'accuracy' or any other metric supported by scikit-learn . With AUC, chance level is the same regardless of class balance. decoding_n_splits: int \u00b6 The number of folds (a.k.a. splits) to use in the cross-validation. n_boot: int \u00b6 The number of bootstrap resamples when estimating the standard error and confidence interval of the mean decoding score.","title":"Statistics"},{"location":"settings/epoched_data/statistics.html#config.contrasts","text":"The conditions to contrast via a subtraction of ERPs / ERFs. Each tuple in the list corresponds to one contrast. The condition names must be specified in conditions above. Pass an empty list to avoid calculation of contrasts. Example Contrast the \"left\" and the \"right\" conditions by calculating left - right at every time point of the evoked responses: conditions = [ 'left' , 'right' ] contrasts = [( 'left' , 'right' )] # Note we pass a tuple inside the list! Contrast the \"left\" and the \"right\" conditions within the \"auditory\" and the \"visual\" modality, and \"auditory\" vs \"visual\" regardless of side: conditions = [ 'auditory/left' , 'auditory/right' , 'visual/left' , 'visual/right' ] contrasts = [( 'auditory/left' , 'auditory/right' ), ( 'visual/left' , 'visual/right' ), ( 'auditory' , 'visual' )]","title":"contrasts"},{"location":"settings/epoched_data/statistics.html#config.decode","text":"Whether to perform decoding (MVPA) on the contrasts specified above as \"contrasts\". MVPA will be performed on the level of individual epochs.","title":"decode"},{"location":"settings/epoched_data/statistics.html#config.decoding_metric","text":"The metric to use for cross-validation. It can be 'roc_auc' or 'accuracy' or any other metric supported by scikit-learn . With AUC, chance level is the same regardless of class balance.","title":"decoding_metric"},{"location":"settings/epoched_data/statistics.html#config.decoding_n_splits","text":"The number of folds (a.k.a. splits) to use in the cross-validation.","title":"decoding_n_splits"},{"location":"settings/epoched_data/statistics.html#config.n_boot","text":"The number of bootstrap resamples when estimating the standard error and confidence interval of the mean decoding score.","title":"n_boot"},{"location":"settings/epoched_data/time_freq.html","text":"time_frequency_conditions: Iterable [ str ] \u00b6 The conditions to compute time-frequency decomposition on. Example time_frequency_conditions = [ 'left' , 'right' ]","title":"Time-frequency analysis"},{"location":"settings/epoched_data/time_freq.html#config.time_frequency_conditions","text":"The conditions to compute time-frequency decomposition on. Example time_frequency_conditions = [ 'left' , 'right' ]","title":"time_frequency_conditions"},{"location":"steps/steps.html","text":"The following table provides a concise summary of each step in the Study Template. You can find the scripts in the scripts directory. Script Description 01-import_and_maxfilter.py Import raw data and apply Maxwell filter. 02-frequency_filter.py Apply low- and high-pass filters. 03-make_epochs.py Extract epochs. 04a-run_ica.py Run Independant Component Analysis (ICA) for artifact correction. 04b-run_ssp.py Run Signal Subspace Projections (SSP) for artifact correction. These are often also referred to as PCA vectors. 05a-apply_ica.py As an alternative to ICA, you can use SSP projections to correct for eye blink and heart beat artifacts. Use either 5a/6a, or 5b/6b. 05b-apply_ssp.py Apply SSP projections and obtain the cleaned epochs. 06-make_evoked.py Extract evoked data for each condition. 07-sliding_estimator.py Running a time-by-time decoder with sliding window. 08-time_frequency.py Running a time-frequency analysis. 09-group_average_sensors.py Make a group average of the time domain data. 10-make_forward.py Compute forward operators. You will need to have computed the coregistration to obtain the -trans.fif files for each subject. 11-make_cov.py Compute noise covariances for each subject. 12-make_inverse.py Compute inverse problem to obtain source estimates. 13-group_average_source.py Compute source estimates average over subjects. 99-make_reports.py Compute HTML reports for each subject.","title":"List of steps"}]}